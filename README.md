# RL Failure Lab ğŸ§ª

> **A modular reinforcement learning experimentation framework for studying failure modes in sequential decision-making.**

[![Python 3.10+](https://img.shields.io/badge/python-3.10+-blue.svg)](https://www.python.org/downloads/)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-ee4c2c.svg)](https://pytorch.org/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

---

## ğŸ¯ What Is This?

**This is not an RL demo.** It's a controlled experimental framework for:

- **Studying failure modes** in RL systems (reward hacking, policy collapse, instability)
- **Comparing algorithms** fairly across environments and reward functions
- **Diagnosing problems** systematically with automatic failure detection
- **Learning RL deeply** by understanding what breaks and why

Perfect for:
- ğŸ“ **Students** learning RL beyond the basics
- ğŸ”¬ **Researchers** studying RL robustness and safety
- ğŸ’¼ **Engineers** building production RL systems

---

## ğŸ—ï¸ Architecture

```
rl-failure-lab/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ environments/        # Parameterized environments
â”‚   â”‚   â”œâ”€â”€ career_env.py    # Career simulation (salary, burnout, switching)
â”‚   â”‚   â””â”€â”€ param_engine.py  # Dynamic environment configuration
â”‚   â”‚
â”‚   â”œâ”€â”€ rewards/             # Versioned reward function registry
â”‚   â”‚   â”œâ”€â”€ short_term.py    # Immediate optimization
â”‚   â”‚   â”œâ”€â”€ long_term_shaped.py  # Potential-based shaping
â”‚   â”‚   â”œâ”€â”€ risk_sensitive.py    # Variance/CVaR penalties
â”‚   â”‚   â”œâ”€â”€ sparse.py        # Goal-only rewards
â”‚   â”‚   â””â”€â”€ delayed.py       # End-of-episode rewards
â”‚   â”‚
â”‚   â”œâ”€â”€ agents/              # RL and baseline agents
â”‚   â”‚   â”œâ”€â”€ dqn_agent.py     # Deep Q-Network
â”‚   â”‚   â”œâ”€â”€ ppo_agent.py     # Proximal Policy Optimization
â”‚   â”‚   â””â”€â”€ heuristic_agents.py  # Rule-based baselines
â”‚   â”‚
â”‚   â”œâ”€â”€ failure_detection/   # Automatic failure detection
â”‚   â”‚   â”œâ”€â”€ reward_hacking.py
â”‚   â”‚   â”œâ”€â”€ policy_collapse.py
â”‚   â”‚   â”œâ”€â”€ nonstationarity.py
â”‚   â”‚   â””â”€â”€ value_explosion.py
â”‚   â”‚
â”‚   â”œâ”€â”€ introspection/       # Policy analysis tools
â”‚   â”‚   â”œâ”€â”€ entropy_tracker.py
â”‚   â”‚   â”œâ”€â”€ state_heatmap.py
â”‚   â”‚   â”œâ”€â”€ counterfactual.py
â”‚   â”‚   â””â”€â”€ phase_plots.py
â”‚   â”‚
â”‚   â””â”€â”€ experiments/         # Experiment infrastructure
â”‚       â”œâ”€â”€ protocol.py      # Reproducible specifications
â”‚       â”œâ”€â”€ runner.py        # Full instrumented execution
â”‚       â””â”€â”€ comparator.py    # Cross-experiment analysis
â”‚
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ demo.py              # Quick start demo
â”‚   â”œâ”€â”€ run_all_experiments.py   # Full benchmark suite
â”‚   â””â”€â”€ stress_tests_simple.py   # Failure mode tests
â”‚
â””â”€â”€ docs/
    â”œâ”€â”€ api_reference.md     # Complete API documentation
    â””â”€â”€ failure_playbook.md  # Debugging guide
```

---

## ğŸš€ Quick Start

### Installation

```bash
# Clone the repository
git clone https://github.com/yourusername/rl-failure-lab.git
cd rl-failure-lab

# Install dependencies
pip install -r requirements.txt

# Install in development mode
pip install -e .
```

### Run the Demo

```bash
python scripts/demo.py
```

### Run Your First Experiment

```python
from src.experiments import ExperimentRunner, ExperimentProtocol

protocol = ExperimentProtocol(
    name="my_first_experiment",
    agent_type="dqn",
    reward_type="short_term_v1",
    n_episodes=500,
)

runner = ExperimentRunner()
result = runner.run(protocol)

print(f"Final reward: {result.final_eval_reward:.2f}")
print(f"Failures detected: {len(result.detected_failures)}")
```

---

## ğŸ“Š Key Features

### 1. Environment Parameterization Engine

Test agent robustness across different conditions:

```python
from src.environments import CareerEnv
from src.environments.param_engine import EnvironmentParams

# Stress test with recession
params = EnvironmentParams.for_regime("recession")
env = CareerEnv(params)

# Or customize parameters
params = EnvironmentParams(
    burnout_rate=0.2,          # High burnout accumulation
    salary_volatility=0.3,     # Volatile market
    switching_risk=0.5,        # Risky job switches
)
```

**Available regimes:** `stable`, `boom`, `recession`, `volatile`

### 2. Versioned Reward Function Registry

10 reward functions with documented failure modes:

| Reward | Behavior | Known Failure Modes |
|--------|----------|---------------------|
| `short_term_v1` | `salary - Î»*burnout` | Reward hacking, myopic |
| `short_term_v2` | Quadratic burnout penalty | Over-penalization |
| `long_term_shaped_v1` | Potential-based shaping | Potential mismatch |
| `risk_sensitive_v1` | Variance penalty | Over-conservative |
| `sparse_v1` | Goal-only reward | Exploration failure |
| `delayed_v1` | Episode-end reward | Credit assignment |

```python
from src.rewards import get_reward, list_rewards

# List all available rewards
print(list_rewards())

# Get a specific reward function
reward_fn = get_reward("short_term_v1", lambda_burnout=1.5)
```

### 3. Automatic Failure Detection

Real-time detection of common RL failures:

```python
from src.failure_detection import (
    CombinedFailureDetector,
    RewardHackingDetector,
    PolicyCollapseDetector,
    ValueExplosionDetector,
)

detector = CombinedFailureDetector([
    RewardHackingDetector(),
    PolicyCollapseDetector(entropy_threshold=0.1),
    ValueExplosionDetector(q_value_threshold=1000),
])

# During training
detector.update(metrics)
failures = detector.detect_all()

for failure in failures:
    print(f"[{failure.severity}] {failure.failure_type}: {failure.description}")
```

### 4. Policy Introspection Tools

Understand what your agent believes:

```python
from src.introspection import EntropyTracker, StateHeatmap, PhasePlotter

# Track exploration over time
entropy_tracker = EntropyTracker()
entropy_tracker.update(agent.get_policy_entropy(), episode)
entropy_tracker.plot(save_path="entropy.png")

# Visualize state visitation
heatmap = StateHeatmap()
for state in episode_states:
    heatmap.record(state[:2], action)
heatmap.plot(include_actions=True, action_names=["STAY", "SWITCH"])

# Plot trajectories in phase space
plotter = PhasePlotter()
plotter.add_trajectory(states, actions, rewards, episode)
plotter.plot_multiple(n_trajectories=20, selection="best")
```

### 5. Human Baselines

Compare RL to simple rules:

```python
from src.agents.heuristic_agents import (
    AlwaysStayAgent,
    BurnoutThresholdAgent,
    ConservativeAgent,
    AdaptiveAgent,
)

# Create a rule-based agent
heuristic = BurnoutThresholdAgent(
    state_dim=4, 
    action_dim=2, 
    burnout_threshold=0.6
)

# Compare to RL
# If RL < heuristic: explain why honestly
```

---

## ğŸ§ª Experiment Results

### Full Benchmark (9 agents Ã— 5 rewards = 45 experiments)

**Top 5 Performers:**

| Rank | Agent | Reward | Score |
|------|-------|--------|-------|
| 1 | **PPO** | short_term_v2 | 179.08 |
| 2 | PPO | short_term_v1 | 178.70 |
| 3 | always_switch | long_term_shaped_v1 | 178.04 |
| 4 | DQN | short_term_v2 | 176.55 |
| 5 | DQN | risk_sensitive_v1 | 176.43 |

**Average by Agent:**
- PPO: 143.84
- DQN: 142.74
- always_switch: 142.34
- random: 1.81
- conservative: -10.99

### Stress Test Results

All 9 tests passed by exhibiting expected failure behaviors:

| Test | Expected Failure | Observed |
|------|------------------|----------|
| T1.1 Salary Trap | Burnout explosion | 85% switches, burnout spike |
| T2.2 Sparse Reward | Credit assignment failure | Dense 175 >> Sparse 9 |
| T3.1 High Scale | Q-value explosion | max_q = 1065 |
| T3.3 Exploration Collapse | Single action dominance | 99.58% one action |
| T4.1 Regime Shift | Performance drop | 175 â†’ 0.16 |

---

## ğŸ“š Documentation

- **[API Reference](docs/api_reference.md)** â€” Complete API documentation
- **[Failure Playbook](docs/failure_playbook.md)** â€” Diagnosis and fixes

### Quick Debugging Reference

| If you see... | It usually means... | Try... |
|---------------|---------------------|--------|
| Action entropy â†’ 0 | Policy collapse | â†‘ entropy coefficient |
| High reward + high burnout | Reward hacking | Redesign reward |
| Oscillating curves | Non-stationarity | â†“ learning rate |
| NaN/Inf values | Value explosion | Gradient clipping |

---

## ğŸ“ What You'll Learn

Using this framework, you'll understand:

1. **Reward Hacking** â€” How agents exploit reward loopholes
2. **Policy Collapse** â€” Why exploration matters
3. **Credit Assignment** â€” The challenge of delayed rewards
4. **Distribution Shift** â€” Why train â‰  test
5. **Baseline Comparison** â€” When complex is worse than simple
6. **Reproducibility** â€” Why seeds matter

---

## ğŸ› ï¸ Running Experiments

### Full Comparison Suite

```bash
python scripts/run_all_experiments.py
```

Runs 45 experiments (9 agents Ã— 5 rewards), generates:
- `experiments/RESULTS_REPORT.md` â€” Summary report
- `experiments/results/full_comparison_*.json` â€” Raw data

### Stress Tests

```bash
python scripts/stress_tests_simple.py
```

Runs failure mode tests:
- T0: Sanity checks
- T1: Reward hacking
- T2: Credit assignment
- T3: Instability
- T4: Non-stationarity
- T7: Baseline comparison
- T8: Reproducibility

---

## ğŸ“ Project Files

```
â”œâ”€â”€ requirements.txt         # Dependencies
â”œâ”€â”€ setup.py                 # Package installation
â”œâ”€â”€ README.md                # This file
â”‚
â”œâ”€â”€ src/                     # Source code
â”‚   â”œâ”€â”€ environments/        # 3 files
â”‚   â”œâ”€â”€ rewards/             # 7 files
â”‚   â”œâ”€â”€ agents/              # 5 files
â”‚   â”œâ”€â”€ failure_detection/   # 6 files
â”‚   â”œâ”€â”€ introspection/       # 5 files
â”‚   â”œâ”€â”€ experiments/         # 5 files
â”‚   â””â”€â”€ utils/               # 4 files
â”‚
â”œâ”€â”€ scripts/                 # Runnable scripts
â”‚   â”œâ”€â”€ demo.py
â”‚   â”œâ”€â”€ run_all_experiments.py
â”‚   â”œâ”€â”€ stress_tests_simple.py
â”‚   â”œâ”€â”€ generate_report.py
â”‚   â””â”€â”€ show_results.py
â”‚
â”œâ”€â”€ docs/                    # Documentation
â”‚   â”œâ”€â”€ api_reference.md
â”‚   â””â”€â”€ failure_playbook.md
â”‚
â””â”€â”€ experiments/             # Results (generated)
    â”œâ”€â”€ RESULTS_REPORT.md
    â””â”€â”€ STRESS_TEST_REPORT.md
```

---

## ğŸ“‹ Requirements

- Python 3.10+
- PyTorch 2.0+
- Gymnasium
- NumPy, Pandas, Matplotlib, Seaborn
- TensorBoard (optional, for logging)
- tqdm, PyYAML

Install all:
```bash
pip install -r requirements.txt
```

---

## ğŸ¤ Contributing

Contributions welcome! Areas of interest:

- [ ] Additional environments (GridWorld, CartPole variants)
- [ ] More failure detectors (gradient pathologies, representation collapse)
- [ ] Visualization dashboard (Streamlit/Gradio)
- [ ] Additional agents (SAC, A2C, model-based)
- [ ] POMDP tests (partial observability)

---

## ğŸ“„ License

MIT License â€” see [LICENSE](LICENSE) for details.

---

## ğŸ™ Acknowledgments

This project was inspired by:
- The OpenAI Gym/Gymnasium project
- Spinning Up in Deep RL
- "Deep Reinforcement Learning that Matters" (Henderson et al.)
- The RL debugging community

---

## ğŸ“¬ Contact

Questions or feedback? Open an issue or reach out!

---

<p align="center">
  <b>Built for learning. Designed for breaking.</b><br>
  <i>Because understanding failure is the path to success.</i>
</p>
